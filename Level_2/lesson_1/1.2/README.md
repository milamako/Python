# Прогнозирование страховых выплат (Insurance Charges Prediction)

Проект посвящён задаче прогнозирования страховых выплат по медицинским полисам на основе демографических и медицинских данных. В ноутбуке реализован полный ML-пайплайн: EDA, фичеинжиниринг, препроцессинг, подбор и сравнение моделей, ансамблирование, анализ ошибок и статистические тесты.

---

## Кратко о проекте
- **Цель:** Прогнозировать расходы на медстраховку (charges) по данным клиента.
- **Данные:** insurance.csv (1338 строк, 7 признаков: возраст, пол, BMI, дети, курение, регион, выплаты).
- **Пайплайн:** EDA → feature engineering → one-hot → масштабирование → train/test split → подбор моделей (RandomForest, XGBoost, CatBoost, LightGBM, PolynomialRegression) → GridSearchCV → ансамбль → анализ ошибок.
- **Модели:** Ансамбли деревьев и полиномиальная регрессия, финальный ансамбль VotingRegressor.
- **Метрики:** RMSE, MAE, R², визуализация ошибок, статистические тесты остатков.
- **Результаты:** R² ансамбля ≈ 0.87, RMSE ≈ 0.34 (log-scale), устойчивые и интерпретируемые предсказания.

---

## Данные

- **Источник:** [Kaggle: Medical Cost Personal Datasets](https://www.kaggle.com/datasets/mirichoi0218/insurance)
- **Формат:** CSV, 7 признаков (age, sex, bmi, children, smoker, region, charges)
- **Целевая переменная:** charges (страховые выплаты)

---

## Этапы работы

1. Загрузка и первичный анализ данных: проверка пропусков, типов, базовые статистики, визуализация распределений.
2. Feature engineering: взаимодействия (bmi_smoker, age_smoker), категоризация возраста и BMI, логарифмирование целевой переменной.
3. Преобразование признаков: one-hot для категориальных, масштабирование числовых (StandardScaler).
4. Разделение данных: train/test split (80/20), без data leakage.
5. Подбор моделей: GridSearchCV для RandomForest, XGBoost, CatBoost, LightGBM, PolynomialRegression.
6. Ансамблирование: VotingRegressor с весами.
7. Оценка: RMSE, MAE, R², визуализация ошибок, тесты Шапиро-Уилка, Бреуша-Пагана, Дарбина-Уотсона.

## Результаты

- Лучшая модель: ансамбль (VotingRegressor)
- R² (test): ~0.87
- RMSE (test, log-scale): ~0.34
- MAE (test, log-scale): ~0.18
- Остатки: не нормальны (p-value Шапиро-Уилка ≈ 0), гетероскедастичности и автокорреляции нет (p-value Бреуша-Пагана ≈ 0.75, Дарбин-Уотсон ≈ 2.14)
- Ошибки распределены неравномерно, есть выбросы на крупных значениях выплат

---

### Требования
- Python 3.10+
- Установить зависимости:

```bash
pip install -r requirements.txt
```
---